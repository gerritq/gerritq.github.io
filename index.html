<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-HZ1ENWVG4C"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-HZ1ENWVG4C');
</script>

  <title>Gerrit Michele Quaremba</title>
  
  <meta name="author" content="Gerrit Michele Quaremba">
  <meta name="viewport" content="width=device-width, initial-scale=1">    
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/IDSIA_logo.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px"><td style="padding:0px">
      <!-- Intro -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
          <td style="padding:2.5%;width:63%;vertical-align:middle">
            <p style="text-align:center">
              <name>Gerrit Michele Quaremba</name>
            </p>
            <p>
              I am a composition and generalization.
          </p>
            <p>
              Before starting my PhD, I received a master's degree from <a href="https://www.bme.hu/">Budapest University of Technology and Economics</a> and worked as a research scientist at <a href="https://aimotive.com/">AImotive</a> on developing self driving cars.
            </p>
            <p style="text-align:center">
              <a href="mailto:q.quaremba@gmail.com">Email</a> &nbsp/&nbsp
              <a href="items/CV_Gerrit_Quaremba.pdf">CV</a> &nbsp/&nbsp
              <a href="https://github.com/gerritq/">GitHub</a>
            </p>
          </td>
          <td style="padding:2.5%;width:40%;max-width:40%">
            <a href="images/csordas.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/csordas_circle.png" class="hoverZoomLink"></a>
          </td>
        </tr>
      </tbody></table>

      <!-- Publications title -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
          </td>
        </tr>
      </tbody></table>
        
      <!-- List of publications -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <!-- MoEUT -->

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
                <img src="images/moeut.svg" width="160", style="padding-top: 2mm;">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2405.16039">
              <papertitle>MoEUT: Mixture-of-Experts Universal Transformers</papertitle>
            </a>
            <br>
            <strong>Róbert Csordás</strong>,
            <a href="http://people.idsia.ch/~kazuki/">Kazuki Irie</a>,
            <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>,
            <a href="https://web.stanford.edu/~cgpotts/">Christopher Potts</a>,
            <a href="https://nlp.stanford.edu/~manning/">Christopher D. Manning</a>
            <br>
              <em>arXiv:2405.16039</em>
            <br>
            <a href="https://arxiv.org/pdf/2405.16039.pdf">pdf</a> /
            <a href="https://github.com/robertcsordas/moeut">code</a> /
            <a href="data/moeut.bib">bib</a>
            <p>
              We propose a novel Mixture-of-Experts Universal Transformer using σ-MoE, SwitchHead, layer grouping, and a novel peri-layernorm. Our method slightly outperforms standard Transformers on language modeling and zero-shot downstream tasks with less compute and memory requirements.
            </p>
          </td>
        </tr> 


        <!-- SwitchHead -->

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
                <img src="images/moeatt_simple.svg" width="160", style="padding-top: 7mm;">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2312.07987">
              <papertitle>SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention</papertitle>
            </a>
            <br>
            <strong>Róbert Csordás</strong>,
            <a href="https://piotrpiekos.github.io/">Piotr Piękos</a>,
            <a href="http://people.idsia.ch/~kazuki/">Kazuki Irie</a>,
            <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
            <br>
              <em>arXiv:2312.07987</em> 
            <br>
            <a href="https://arxiv.org/pdf/2312.07987.pdf">pdf</a> /
            <a href="https://github.com/robertcsordas/moe_attention">code</a> /
            <a href="data/moe_attention.bib">bib</a>
            <p>
              We propose a novel MoE attention, which can match the performance of parameter-matched dense models with a fraction of the compute and memory requirements. We also present the "SwitchAll" model, where each layer is an MoE.
            </p>
          </td>
        </tr> 

        <!-- Approximating Two-Layer Feedforward Networks for Efficient Transformers -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
                <img src="images/moe.png" width="160", style="padding-top: 7mm;">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2310.10837">
              <papertitle>Approximating Two-Layer Feedforward Networks for Efficient Transformers</papertitle>
            </a>
            <br>
            <strong>Róbert Csordás</strong>,
            <a href="http://people.idsia.ch/~kazuki/">Kazuki Irie</a>,
            <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
            <br>
              <em>EMNLP Findings 2023</em> 
            <br>
            <a href="https://arxiv.org/pdf/2310.10837.pdf">pdf</a> /
            <a href="https://github.com/robertcsordas/moe">code</a> /
            <a href="data/moe.bib">bib</a>
            <p>
              We present different approximation methods for two-layer feedforward networks in a unified framework. Based on this, we develop a better-performing MoE, which matches or even outperforms the parameter-equivalent dense models.
            </p>
          </td>
        </tr> 


        <!-- Topological Neural Discrete Representation Learning à la Kohonen -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
                <img src="images/kohonen.jpg" width="160", style="padding-top: 7mm;">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2302.07950">
              <papertitle>Topological Neural Discrete Representation Learning à la Kohonen</papertitle>
            </a>
            <br>
            <a href="http://people.idsia.ch/~kazuki/">Kazuki Irie*</a>,
            <strong>Róbert Csordás*</strong>,
            <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
            <br>
              <em>International Conference on Artificial Neural Networks (ICANN), 2024</em>
            <br>
            <a href="https://arxiv.org/pdf/2302.07950.pdf">pdf</a> /
            <a href="https://github.com/IDSIA/kohonen-vae">code</a> /
            <a href="data/kohonen.bib">bib</a>
            <p>
              We show that vector quantization is a special case of self-organizing maps (SOMs). Using the SOM formulation proposed by Kohonen in his 1982 paper improves converge speed, makes the training more robust, and results in a topologically organized representation space.
            </p>
          </td>
        </tr> 

        <!-- Randomized Pos Enc -->

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
                <img src="images/randpos.png" width="160", style="padding-top: 7mm;">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2305.16843">
              <papertitle>Randomized Positional Encodings Boost Length Generalization of Transformers</papertitle>
            </a>
            <br>
            <a href="https://twitter.com/anianruoss">Anian Ruoss</a>,
            <a href="https://twitter.com/gregdeletang">Gregoire Deletang</a>,
            <a href="http://tim.inversetemperature.net/"">Tim Genewein</a>,
            <a href="https://sites.google.com/view/graumoya/home">Jordi Grau-Moya</a>, 
            <strong>Róbert Csordás</strong>,
            <a href="https://scholar.google.com/citations?user=JbGXRUIAAAAJ">Mehdi Bennani</a>, 
            <a href="https://twitter.com/ShaneLegg">Shane Legg</a>,
            <a href="https://jveness.info/">Joel Veness</a>
            <br>
              <em>Annual Meeting of the Association for Computational Linguistics (ACL), 2023</em> 
            <br>
            <a href="https://arxiv.org/pdf/2305.16843.pdf">pdf</a> /
            <a href="data/randpos.bib">bib</a>
            <p>
              We show that the transformer's poor length generalization is linked to the positional encodings being out-of-distribution. We introduce a novel positional encoding that samples a randomized ordered subset of sinusoidal positional encodings. We show the befit of this positional encoding on various algorithmic tasks.
            </p>
          </td>
        </tr> 

        <!-- CTL++ -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ctlpp.svg" width="160">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2210.06350">
              <papertitle>CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations</papertitle>
            </a>
            <br>
            <strong>Róbert Csordás</strong>,
            <a href="http://people.idsia.ch/~kazuki/">Kazuki Irie</a>,
            <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
            <br>
              <em>Empirical Methods in Natural Language Processing (EMNLP), 2022 </em>
            <br>
            <a href="https://arxiv.org/pdf/2210.06350.pdf">pdf</a> /
            <a href="https://github.com/robertcsordas/ctlpp">code</a> /
            <a href="data/ctlpp_poster.pdf">poster</a> /
            <a href="data/ctlpp.bib">bib</a>
            <br>
            <p>
              We developed a new dataset for testing systematicity based on CTL by partitioning the data based on functional groups. Using this, we were able to show that Transformers naturally learn multiple, incompatible representations of the same symbol. As a result, the network fails when the symbol is fed to a function that has not seen that specific representation before.
            </p>
          </td>
        </tr> 
         
        <!-- A Generalist Neural Algorithmic Learner -->
         <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
                <img src="images/clrs.svg" width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2209.11142">
              <papertitle>A Generalist Neural Algorithmic Learner</papertitle>
            </a>
            <br>
            Borja Ibarz,
            <a href="https://yobibyte.github.io/">Vitaly Kurin</a>,
            <a href="https://twitter.com/gpapamak">George Papamakarios</a>,
            <a href="https://twitter.com/k_nikiforou">Kyriacos Nikiforou</a>,
            <a href="https://twitter.com/MehdiBennaniML">Mehdi Bennani</a>,
            <strong>Róbert Csordás</strong>,
            <a href="https://twitter.com/andrewdudzik">Andrew Dudzik</a>,
            <a href="https://matko.info/">Matko Bošnjak</a>,
            Alex Vitvitskyi,
            <a href="http://www.cs.toronto.edu/~rubanova/">Yulia Rubanova</a>,
            <a href="https://andreeadeac22.github.io/">Andreea Deac</a>,
            <a href="https://beabevi.github.io/">Beatrice Bevilacqua</a>,
            <a href="http://yaroslav.ganin.net/">Yaroslav Ganin</a>,
            <a href="https://www.gatsby.ucl.ac.uk/~ucgtcbl/">Charles Blundell</a>,
            <a href="https://petar-v.com/">Petar Veličković</a>
            <br>
              <em>Learning on Graphs (LoG), 2022</em> 
            <br>
            <a href="https://arxiv.org/pdf/2209.11142.pdf">pdf</a> /
            <a href="https://github.com/deepmind/clrs">code</a> /
            <a href="data/generalist_clrs.bib">bib</a>
            <p>
              We train multi-task generalist reasoning architecture on the CLRS algorithmic reasoning benchmark that shares a single, universal processor among all tasks. Furthermore, we introduce numerous improvements to the previous best architecture, achieving new SOTA even in the single-task case.
            </p>
          </td>
         </tr> 
         <!-- The Dual Form of Neural Networks Revisited -->
         <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
                <img src="images/dual_form.svg" width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2202.05798">
              <papertitle>The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention</papertitle>
            </a>
            <br>
            <a href="http://people.idsia.ch/~kazuki/">Kazuki Irie*</a>,
            <strong>Róbert Csordás*</strong>,
            <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
            <br>
              <em>International Conference on Machine Learning (ICML), 2022</em> 
            <br>
            <a href="https://arxiv.org/pdf/2202.05798.pdf">pdf</a> /
            <a href="https://github.com/robertcsordas/linear_layer_as_attention">code</a> /
            <a href="data/dual_form.bib">bib</a>
            <p>
              Linear layers in neural networks (NNs) trained by gradient descent can be expressed as a key-value memory system which stores all training datapoints and the initial weights, and
              produces outputs using unnormalised dot attention over the entire training experience. While this has been technically known since the ’60s, no prior work has effectively studied the
              operations of NNs in such a form. We conduct experiments on this dual formulation and study the potential of directly visualising how an NN makes use of training patterns at test time,
              as well as its limitations.
            </p>
          </td>
        </tr> 

        <!-- A Modern Self-Referential Weight Matrix -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/srwm.svg" width="160">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://openreview.net/forum?id=lVUGfLpNpCF">
              <papertitle>A Modern Self-Referential Weight Matrix That Learns to Modify Itself</papertitle>
            </a>
            <br>
            <a href="http://people.idsia.ch/~kazuki/">Kazuki Irie</a>,
            <a href="https://ischlag.github.io/">Imanol Schlag</a>,
            <strong>Róbert Csordás</strong>,
            <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
            <br>
              <em>International Conference on Machine Learning (ICML), 2022</em> 
            <br>
            <a href="https://openreview.net/pdf?id=lVUGfLpNpCF">pdf</a> /
            <a href="https://github.com/IDSIA/modern-srwm">code</a> /
            <a href="data/modern_self_ref.bib">bib</a>
            <br>
            <p>
              The weight matrix (WM) of a neural network (NN) is its program which remains fixed after training. The WM or program of a self-referential NN, however, can keep rapidly modifying all of itself during runtime. In principle, such NNs are capable of recursive self-improvement. Here we propose a scalable self-referential WM (SRWM) that uses self-generated training patterns, outer products and the delta update rule to modify itself.
            </p>
          </td>
        </tr> 

        <!-- Neural Data Router -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
                <img src="images/ndr.svg" width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2110.07732">
              <papertitle>The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization</papertitle>
            </a>
            <br>
            <strong>Róbert Csordás</strong>,
            <a href="http://people.idsia.ch/~kazuki/">Kazuki Irie</a>,
            <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
            <br>
              <em>International Conference on Learning Representations (ICLR), 2022</em> 
            <br>
            <a href="https://arxiv.org/pdf/2110.07732.pdf">pdf</a> /
            <a href="https://github.com/robertcsordas/ndr">code</a> /
            <a href="https://docs.google.com/presentation/d/1CdcyU2-PpztTSaiRbXEI3xBYUtTycc6qv5xGdl8CIOI/edit?usp=sharing">slides</a> /
            <a href="data/ndr_poster.pdf">poster</a> /
            <a href="data/ndr.bib">bib</a>
            <p>
              We look at Transformers as a system for routing relevant information to the right node/operation at the right time in the grid represented by its column.
              To facilitate learning useful control flow, we propose two modifications to the Transformer architecture: copy gate and geometric attention. The resulting
              Neural Data Router (NDR) architecture achieves length generalization compositional table lookup task, as well as generalization across computational depth
              on the simple arithmetic task and ListOps.
            </p>
          </td>
        </tr> 

        <!-- Devil is in the details -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
                <img src="images/trafo_gen.svg" width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2108.12284">
              <papertitle>The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers</papertitle>
            </a>
            <br>
            <strong>Róbert Csordás</strong>,
            <a href="http://people.idsia.ch/~kazuki/">Kazuki Irie</a>,
            <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
            <br>
              <em>Empirical Methods in Natural Language Processing (EMNLP), 2021 </em>
            <br>
            <a href="https://arxiv.org/pdf/2108.12284.pdf">pdf</a> /
            <a href="https://github.com/robertcsordas/transformer_generalization">code</a> /
            <a href="https://docs.google.com/presentation/d/1YOMssLAUo7IFqu2-wePWCNiEIb9vsOmPDUDU9toGCjo/edit?usp=sharing">slides</a> /
            <a href="data/poster_devil_is_in_the_details.pdf">poster</a> /
            <a href="data/devil_in_the_details.bib">bib</a>
            <p>
              We improve the systematic generalization of Transformers on SCAN (0 -> 100% with length cutoff=26), CFQ (66 -> 81% on output length split),
               PCFG (50 -> 85% on productivity split, 72 -> 96% on systematicity split), COGS (35 -> 81%), and Mathematics dataset, by revisiting model configurations as basic
               as scaling of embeddings, early stopping, relative positional embedding, and weight sharing. We also show that relative positional embeddings largely mitigate the EOS
                decision problem. Importantly, differences between these models are typically invisible on the IID data split, which calls for proper generalization validation sets. 
            </p>
          </td>
        </tr> 

        <!-- Fast weights -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
                <img src="images/fastweights.svg" width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2106.06295">
              <papertitle>Going Beyond Linear Transformers with Recurrent Fast Weight Programmers</papertitle>
            </a>
            <br>
            <a href="http://people.idsia.ch/~kazuki/">Kazuki Irie</a>,
            <a href="https://ischlag.github.io/">Imanol Schlag</a>,
            <strong>Róbert Csordás</strong>,
            <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
            <br>
              <em>Conference on Neural Information Processing Systems (NeurIPS), 2021</em>
            <br>
            <a href="https://arxiv.org/pdf/2106.06295.pdf">pdf</a> /
            <a href="https://github.com/IDSIA/recurrent-fwp">code</a> /
            <a href="data/fwtrafo.bib">bib</a>
            <p>
              Inspired by the effectiveness of Fast Weight Programmers in the context of Linear Transformers, in this work we explore the recurrent Fast Weight Programmers (FWPs), which exhibit advantageous properties of both Transformers and RNNs.
            </p>
          </td>
        </tr> 

        <!-- Are neural networks modular -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/modular.svg" width="160">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://openreview.net/pdf?id=7uVcpu-gMD">
              <papertitle>Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks</papertitle>
            </a>
            <br>
            <strong>Róbert Csordás</strong>,
            <a href="http://www.sjoerdvansteenkiste.com/">Sjoerd van Steenkiste</a>,
            <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
            <br>
              <em>International Conference on Learning Representations (ICLR), 2021</em>
            <br>
            <a href="https://openreview.net/pdf?id=7uVcpu-gMD">pdf</a> /
            <a href="https://github.com/RobertCsordas/modules">code</a> /
            <a href="https://docs.google.com/presentation/d/1PUMRh1WRmlbWeMmcTWjgPWLAWebwj9IcDYwfcDSY024/edit?usp=sharing">slides</a> /
            <a href="data/modules_poster.pdf">poster</a> /
            <a href="data/modules.bib">bib</a>
            <br>
            <p>
              This paper presents a novel method based on learning binary weight masks to identify individual weights and subnets responsible for specific functions. We contribute an extensive study of emerging modularity in NNs that covers several standard architectures and datasets using this powerful tool. We demonstrate how common NNs fail to reuse submodules and offer new insights into systematic generalization on language tasks.
            </p>
          </td>
        </tr> 

        <!-- DNC -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/dnc.svg" width="160">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/1904.10278.pdf">
              <papertitle>Improving Differentiable Neural Computers Through Memory Masking, De-allocation, and Link Distribution Sharpness Control</papertitle>
            </a>
            <br>
            <strong>Róbert Csordás</strong>,
            <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
            <br>
              <em>International Conference on Learning Representations (ICLR), 2019</em>
            <br>
              <em>NeurIPS Workshop on Relational Representation Learning, 2018</em>
            <br>
            <a href="https://arxiv.org/pdf/1904.10278.pdf">pdf</a> /
            <a href="https://github.com/robertcsordas/dnc">code</a> /
            <a href="https://docs.google.com/presentation/d/1mn3AwYIzlAL3veoLGV2evKdT5d2e9QCzsORKXYK54i0/edit?usp=sharing">slides</a> /
            <a href="data/poster_dnc.pdf">poster</a> /
            <a href="data/dnc.bib">bib</a>
            <p>
              We propose three improvements for the DNC architecture, which significantly improves its performance on algorithmic reasoning tasks. First, the lack of key-value separation makes the address distribution dependent also on the stored value.  Second, 
              DNC leaves deallocated data in the memory, which results in aliasing. Third, the temporal linkage matrix quickly degrades the sharpness of the address distribution. Our proposed fixes improve the mean error rate on the bAbI question answering dataset by 43%.
            </p>
          </td>
        </tr>
      </tbody></table>


      <!-- Workshop papers title -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Workshop papers</heading>
          </td>
        </tr>
      </tbody></table>

      <!-- Workshop papers list -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <!-- Improving Baselines in the Wild -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/wilds.png" width="160">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://openreview.net/forum?id=9vxOrkNTs1x">
              <papertitle>Improving Baselines in the Wild</papertitle>
            </a>
            <br>
            <a href="http://people.idsia.ch/~kazuki/">Kazuki Irie</a>,
            <a href="https://ischlag.github.io/">Imanol Schlag</a>,
            <strong>Róbert Csordás</strong>,
            <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
            <br>
              <em>NeurIPS DistShift Workshop, 2021</em>  
            <br>
            <a href="https://openreview.net/pdf?id=9vxOrkNTs1x">pdf</a> /
            <a href="https://github.com/kazuki-irie/fork--wilds-public">code</a>
            <br>
            <p>
              We present our critical observations on the iWildCam and FMoW datasets of the recently released WILDS benchmark. We show that (1) Conducting separate cross-validation for each evaluation metric is crucial for both datasets, (2) A weak correlation between validation and test performance might make model development difficult for iWildCam, (3) Minor changes in the training of hyper-parameters improve the baseline by a relatively large margin, (4) There is a strong correlation between certain domains and certain target labels.
            </p>
          </td>
        </tr> 
      </tbody></table>

      <!-- Patents title -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Patents</heading>
          </td>
        </tr>
      </tbody></table>

      <!-- Patents list -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/patent.png" width="160">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://pimg-fpiw.uspto.gov/fdd/53/807/103/0.pdf">
              <papertitle>Method and apparatus for generating a displacement map of an input dataset pair</papertitle>
            </a>
            <br>
            <strong>Róbert Csordás</strong>,
            Ágnes Kis-Benedek,
            Balázs Szalkai
            <br>
              <em>US Patent 10,380,753</em>
            <br>
            <a href="https://pimg-fpiw.uspto.gov/fdd/53/807/103/0.pdf">pdf</a>
            <p>
              We propose a fast and accurate method for generating displacement maps from stereo image pairs using neural networks. This enables 
              more robust depth prediction compared to standard methods.
            </p>
          </td>
        </tr>
      </tbody></table>

      <!-- Talks -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Talks</heading>
                <ul>
                  <li> In May 2024 I gave a <a href="https://docs.google.com/presentation/d/1_k9gklQ9FouZURy-HBI6HTG1HzRXiSXF-dsU4UVp17A/edit?usp=sharing">talk</a> in <a href="https://www.apple.com/">Apple</a> about my work</li>
                </ul>
                <ul>
                  <li> In May 2024 I gave a <a href="https://docs.google.com/presentation/d/1mv8-X077NBqofKc8Hd7LW0_dE-cbK7qCZiQmiQhZ8As/edit?usp=sharing">talk</a> in <a href="https://www.adobe.com/">Adobe</a> on Mixture-of-Experts with focus on Universal Transformers</li>
                </ul>
                <ul>
                  <li> In March 2024 I gave a <a href="https://docs.google.com/presentation/d/1YPU9JFo818fXmPhh1RX_a0F3VlF_ij1ScJr3-vsUo1g/edit?usp=sharing">talk</a> in <a href="https://www.nvidia.com/">Nvidia</a> about my work</li>
                </ul>
                <ul>
                  <li> In March 2023 I gave a <a href="https://docs.google.com/presentation/d/1LU7qT2uNGsGRww50iZkiT-3Yb8NNjQ0_3rlND1kQM8U/edit?usp=sharing">talk</a> at <a href="https://nlp.epfl.ch/">NLP group</a> of <a href="https://www.epfl.ch/">EPFL</a> about my work.</li>
                </ul>
                <ul>
                  <li> In November 2022 I gave a <a href="https://docs.google.com/presentation/d/1LU7qT2uNGsGRww50iZkiT-3Yb8NNjQ0_3rlND1kQM8U/edit?usp=sharing">talk</a> at <a href="https://rycolab.io/">Rycolab</a> at <a href="https://ethz.ch">ETH Zürich</a> on the Neural Sequence Models Theory group on how ideas from compositionality improve systematic generalization.</li>
                </ul>
                <ul>
                  <li> In June 2022 I gave a <a href="https://docs.google.com/presentation/d/1eXwP3vqfZIsvd31aqciJfaav96CIMi9o_z6_vud6j98/edit?usp=sharing">talk</a> on the Neural Sequence Models Theory group on how ideas from compositionality improve systematic generalization.</li>
                </ul>
                <ul>
                  <li> In June 2022 I gave a <a href="https://docs.google.com/presentation/d/1mVM6p-48yrES696YiCgouQeJxAli3ZATsisFSXknMD0/edit?usp=sharing">talk</a> on the <a href="https://nlp.stanford.edu/seminar/">Stanford NLP Seminar</a> on how ideas from compositionality improve systematic generalization.</li>
                </ul>
                <ul>
                  <li> In April 2022 I gave a <a href="https://docs.google.com/presentation/d/1Y5vP0ONiQX_xtOrLo4f1rZpm_ysAVKhpupcYv_CwrpA/edit?usp=sharing">talk</a> for <a href="https://www.mit.edu/~jda/">Jacob Andreas</a>' <a href="https://lingo.csail.mit.edu/people.html">group</a> on how ideas from compositionality improve systematic generalization.</li>
                </ul>
                <ul>
                  <li> In March 2022 I gave a <a href="https://webcast.kaust.edu.sa/Mediasite/Showcase/default/Presentation/7f00f80ef622453286444c471f360bbd1d">talk</a> on the <a href="https://cemse.kaust.edu.sa/ai/aii-symp-2022">Rising Stars in AI Symposium</a> at KAUST on how ideas from compositionality improve systematic generalization.</li>
                </ul>
                <ul>
                  <li> In October 2021 I gave a <a href="https://www.youtube.com/watch?v=_r3135p-ksQ">remote talk</a> together with <a href="http://people.idsia.ch/~kazuki/">Kazuki Irie</a> at the MIAI Deeptails Seminar organized by INRIA Grenoble Rhône-Alpes on our paper <a href="https://arxiv.org/abs/2108.12284">The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers</a>.</li>
                </ul>
                <ul>
                    <li> In July 2021 I gave a remote talk at the Center for Machine Learning and Neuroscience at the Baylor college of Medicine in Houston on the paper <a href="https://openreview.net/pdf?id=7uVcpu-gMD">Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks</a>.</li>
                </ul>
            </td>
        </tr>
      </table>
    </td></tr>
  </tbody></table>

  <!-- Credits -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:0px">
        <br>
        <p style="text-align:right;font-size:small;">
          Website template credits to <a href="https://jonbarron.info">Jon Barron</a>.
        </p>
      </td>
    </tr>
  </tbody></table>
</body>

</html>
